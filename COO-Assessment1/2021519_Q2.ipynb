{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTH377 - Convex Optimisation (COO) - Assessment 1\n",
    "\n",
    "### Problem 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have been given the function f(x,y):\n",
    "$$\n",
    "f(x,y) = (x^2 - 3y^2)^2 + \\sin^2(x^2+y^2)\n",
    "$$\n",
    "\n",
    "- We need to minimise this function as an unconstrained optimisation problem using the combination descent algorithm with starting point at (1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function\n",
    "def f(x,y):\n",
    "    return (np.sin(x**2 + y**2))**2 + (x**2 - 3*(y**2))**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the gradient, we need expressions for the partial derivates of f(x,y) wrt x and y:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 4x(x^2-3y^2) + 2x\\sin(2(x^2 + y^2))\n",
    "\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = -12y(x^2-3y^2) + 2y\\sin(2(x^2 + y^2))\n",
    "$$\n",
    "\n",
    "- Thus the gradient of f(x,y) is a vector of the form:\n",
    "\n",
    "$$\n",
    "\\nabla f = \\begin{vmatrix} \n",
    "\\frac{\\partial f}{\\partial x} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial f}{\\partial y}\n",
    "\\end{vmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient\n",
    "def gradient_f(x,y):\n",
    "\n",
    "    # Implementing the partial derivatives as derived above\n",
    "    df_dx = 4*x*(x**2 - 3*(y**2)) + 4*(np.sin(x**2 + y**2))*(np.cos(x**2 + y**2))*x\n",
    "    df_dy = (-12)*y*(x**2 - 3*(y**2)) + 4*(np.sin(x**2 + y**2))*(np.cos(x**2 + y**2))*y\n",
    "\n",
    "    # Returning the gradient as a numpy array of the partial derivatives\n",
    "    return np.array([df_dx, df_dy])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the Hessian Matrix, we need the double partial derivates with respect to x and y. Calculated them as follows:\n",
    "$$ \\frac{\\partial^{2} f}{\\partial x^2} = 12x^2 - 12y^2 +2\\sin(2(x^2 + y^2)) + 8x^2\\cos(2(x^2 + y^2)) $$\n",
    "$$ \\frac{\\partial^{2} f}{\\partial xy} = \\frac{\\partial^{2} f}{\\partial yx} = -24xy + 8xy\\cos(2(x^2 + y^2)) $$\n",
    "$$ \\frac{\\partial^{2} f}{\\partial y^2} = 108y^2 - 12x^2 +2\\sin(2(x^2 + y^2)) + 8y^2\\cos(2(x^2 + y^2)) $$\n",
    "\n",
    "- Thus the Hessian Matrix of the given function f(x,y) at any (x,y) will be:\n",
    "$$\n",
    "H(x,y) = \\begin{vmatrix} \n",
    "\\frac{\\partial^{2} f}{\\partial x^2}    \\frac{\\partial^{2} f}{\\partial xy} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial^{2} f}{\\partial yx}     \\frac{\\partial^{2} f}{\\partial y^2} \n",
    "\\end{vmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hessian\n",
    "def hessian_f(x,y):\n",
    "\n",
    "    # Implementing the expressions as derived above\n",
    "    df2_dxdx = 12*(x**2) - 12*(y**2) +2*(np.sin(2*(x**2 + y**2))) + (8*(x**2))*np.cos(2*(x**2 + y**2))\n",
    "    df2_dxdy = -24*x*y + (8*x*y)*np.cos(2*(x**2 + y**2))\n",
    "    df2_dydy = 108*(y**2) - 12*(x**2) +2*(np.sin(2*(x**2 + y**2))) + (8*(y**2))*np.cos(2*(x**2 + y**2))\n",
    "\n",
    "    # Returning the Hessian as a 2x2 matrix\n",
    "    return np.array([[df2_dxdx, df2_dxdy], [df2_dxdy, df2_dydy]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We find the descent direction 'd' using the Backtracking-Linear Search Routine or the Armijo Rule.\n",
    "- We check for the following inequality to be true:\n",
    "$$\n",
    "f(x) - f(x+td) < \\alpha(-Df(x)Â·(td))\n",
    "$$\n",
    "where $\\alpha$ is an acceptable descent parameter $\\in (0,0.5)$\n",
    "\n",
    "- We start with t = 1 and then till the about condition holds true, we multiply t with a reduction factor $\\beta$ $\\in (0,1)$\n",
    "\n",
    "- I have implemented the same below. In our case $d$ would be a 1x2 matrix and $Df(x)$ becomes the gradient $\\nabla f$ at the original point $x,y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Backtracking Line Search\n",
    "def linear_search(d, alpha, beta, point):\n",
    "    t = 1\n",
    "    while((f(point[0], point[1]) - f(point[0] + t*d[0], point[1] + t*d[1])) < (-alpha*t*np.dot(gradient_f(point[0], point[1]), d))): \n",
    "        t = beta*t\n",
    "    return t\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We implement the Combination Descent Algorithm below.\n",
    "- First, we have a function `is_pos_def` which checks if a matrix is positive definite by seeing if all its eigenvalues are positive. \n",
    "- This works for a symmetric matrix and we can do this test on the Hessian Matrix $Hf(x)$ because the Hessian Matrix is always symmetric as $\\frac{\\partial^{2} f}{\\partial xy} = \\frac{\\partial^{2} f}{\\partial yx}$ (in the context of $R^2$)\n",
    "\n",
    "- If our matrix is positive definite, we choose our $d = (Hf(x))^{-1}(-\\nabla f(x))$ or the $d = -\\nabla f(x)$\n",
    "- Then we apply the Linear Search method to the step-size $t$ which we then use with the desent direction $d$ to update our x and y coordinates.\n",
    "- We perform this in a loop till the value of $\\nabla f(x) \\leq \\eta $ where $\\eta$ is the tolerance level set by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a matrix is positive definite\n",
    "def is_pos_def(x):\n",
    "\n",
    "    # For checking if a matrix is positive definite we check if all the eigenvalues are positive (applicable for symmetric matrices)\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "\n",
    "# Implementing the Combination Descent Method\n",
    "def combination_descent(x0, y0, alpha, beta, epsilon, max_iters = 100):\n",
    "    # Initialize the variables\n",
    "    x = x0\n",
    "    y = y0\n",
    "\n",
    "    #Initializing the loop\n",
    "    while (np.linalg.norm(gradient_f(x,y)) > epsilon) and (max_iters != 0):\n",
    "        \n",
    "        # Compute the gradient at the current point (x,y)\n",
    "        grad = gradient_f(x,y)\n",
    "\n",
    "        # Compute the Hessian at the current point (x,y)\n",
    "        hess = hessian_f(x,y)\n",
    "\n",
    "        # Combination Descent Condition:\n",
    "        if(is_pos_def(hess)):\n",
    "\n",
    "            # If the Hessian is positive definite, we use the Newton Step\n",
    "            \n",
    "            # Computing the inverse of the Hessian\n",
    "            hess_inverse = np.linalg.inv(hess)\n",
    "\n",
    "            # Computing the Newton Step by taking the negative of the Hessian inverse dotted with the gradient vector\n",
    "            d = -np.dot(hess_inverse, grad)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # If the Hessian is not positive definite, we use the Gradient Descent Step which is just the negative of the gradient vector\n",
    "            d = -grad\n",
    "\n",
    "        # Compute the step size\n",
    "        t = linear_search(d, alpha, beta, [x,y])\n",
    "\n",
    "        # Update the variables\n",
    "        x = x + t*d[0]\n",
    "        y = y + t*d[1]\n",
    "\n",
    "        # Update the number of iterations\n",
    "        max_iters = max_iters - 1\n",
    "\n",
    "    # Return the solution as a numpy array with the number of iterations it took and the coordinates of the solution point\n",
    "    return np.array([x, y, max_iters])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we define the following values:\n",
    "$$\n",
    "(x_0,y_0) = (1,1)\n",
    "$$\n",
    "$$\n",
    "\\alpha = 0.1\n",
    "$$\n",
    "$$\n",
    "\\beta = 0.3\n",
    "$$\n",
    "$$\n",
    "\\eta = 10^{-7}\n",
    "$$\n",
    "- We then start the combination descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point of minima for the given function is at: (1.5350 , 0.8862)\n",
      "The approximate minimum value of the given function is: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the initial point\n",
    "x0 = 1\n",
    "y0 = 1\n",
    "\n",
    "# Defining Constants\n",
    "alpha = 0.2\n",
    "beta = 0.5\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Calling the function\n",
    "point = combination_descent(x0, y0, alpha, beta, epsilon)\n",
    "\n",
    "# Printing the results\n",
    "print(\"The point of minima for the given function is at: (%.4f , %.4f)\" %(point[0], point[1]))\n",
    "print(\"The approximate minimum value of the given function is: %.4f\" %(f(point[0], point[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get the minima approximately for $f(x,y)$ at $(0.0021, 0.0009)$.\n",
    "- The approximate minimum value of the function hence is $0.00$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
