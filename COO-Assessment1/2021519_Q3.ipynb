{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTH377 - Convex Optimisation (COO) - Assessment 1\n",
    "\n",
    "### Problem 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this problem, we have defined set S as a subset of $R^n$ and is defined by a collection of inequalities:\n",
    "\n",
    "- S = {x $\\in \\mathbb{R}^n$: for every $j = 1,.....,m$,   $g_j(x) \\geq 0$}\n",
    "\n",
    "- With any set S, we have defined a potential function:\n",
    "$$\n",
    "\\Psi (x) = -\\sum_{j=1}^{+m} \\log g_{j}(x)\n",
    "$$\n",
    "\n",
    "- The analytical center is defined as the vector $x$ which minimises the potential function defined above.\n",
    "\n",
    "- We have been given an example instance in $\\mathbb{R}^2$ where we have been three inequalities as defined below:\n",
    "$$\n",
    "g_1(x_1,x_2) = 2x_2 - x_1\n",
    "$$\n",
    "$$\n",
    "g_2(x_1,x_2) = 2x_1 - x_2\n",
    "$$\n",
    "$$\n",
    "g_3(x_1,x_2) = 1 - x_2 - x_1\n",
    "$$\n",
    "- We need to compute the analytical center by framing this as an unconstrained optimisation problem with starting point as $(0.25, 0.25)$ and applying the combination descent algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our potential function would be of the form:\n",
    "$$\n",
    "\\Psi (x_1,x_2) = \\log (\\frac {1}{(2x_2-x_1)(2x_1 - x_2)(1 - x_1 - x_2)})\n",
    "$$\n",
    "\n",
    "- We can take this as our $f(x_1, x_2)$ to be minimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Define the function\n",
    "def f(x1,x2):\n",
    "    return (-1)*(math.log((2*x2-x1)*(2*x1-x2)*(1-x1-x2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the gradient, we need expressions for the partial derivates of f(x,y) wrt x and y calculated below:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1} = \\frac {1}{2x_2-x_1} + \\frac {-2}{2x_1-x_2} + \\frac {1}{1-x_1-x_2}\n",
    "\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_2} = \\frac {-2}{2x_2-x_1} + \\frac {1}{2x_1-x_2} + \\frac {1}{1-x_1-x_2}\n",
    "$$\n",
    "\n",
    "- Thus the gradient of f(x,y) is a vector of the form:\n",
    "\n",
    "$$\n",
    "\\nabla f = \\begin{vmatrix} \n",
    "\\frac{\\partial f}{\\partial x} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial f}{\\partial y}\n",
    "\\end{vmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient\n",
    "def gradient_f(x1,x2):\n",
    "\n",
    "    # Taking x1 as x and x2 as y\n",
    "    df_dx = (1/(2*x2-x1)) + (-2/(2*x1-x2)) + (1/(1-x1-x2))\n",
    "    df_dy = (-2/(2*x2-x1)) + (1/(2*x1-x2)) + (1/(1-x1-x2))\n",
    "\n",
    "    # Return the gradient as a numpy array\n",
    "    return np.array([df_dx,df_dy])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the Hessian Matrix, we need the double partial derivates with respect to x and y. Calculated them as follows:\n",
    "$$ \\frac{\\partial^{2} f}{\\partial {x_1}^2} = \\frac {1}{(2x_2-x_1)^2} + \\frac {4}{(2x_1-x_2)^2} + \\frac {1}{(1-x_1-x_2)^2} $$\n",
    "$$ \\frac{\\partial^{2} f}{\\partial {{x_2}^2}} = \\frac {4}{(2x_2-x_1)^2} + \\frac {1}{(2x_1-x_2)^2} + \\frac {1}{(1-x_1-x_2)^2} $$\n",
    "$$ \\frac{\\partial^{2} f}{\\partial {x_1x_2}} = \\frac {-2}{(2x_2-x_1)^2} + \\frac {-2}{(2x_1-x_2)^2} + \\frac {1}{(1-x_1-x_2)^2} $$\n",
    "\n",
    "- Thus the Hessian Matrix of the given function f(x,y) at any (x,y) will be:\n",
    "$$\n",
    "H(x_1,x_2) = \\begin{vmatrix} \n",
    "\\frac{\\partial^{2} f}{\\partial {x_1}^2}   \\frac{\\partial^{2} f}{\\partial {x_1x_2}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial^{2} f}{\\partial {x_2x_1}}   \\frac{\\partial^{2} f}{\\partial {{x_2}^2}} \n",
    "\\end{vmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hessian\n",
    "def hessian_f(x1, x2):\n",
    "\n",
    "    # Taking x1 as x and x2 as y\n",
    "    d2f_dx2 = (1/((2*x2-x1)**2)) + (4/((2*x1-x2)**2)) + (1/((1-x1-x2)**2))\n",
    "    d2f_dy2 = (4/((2*x2-x1)**2)) + (1/((2*x1-x2)**2)) + (1/((1-x1-x2)**2))\n",
    "    d2f_dxdy = (-2/((2*x2-x1)**2)) + (-2/((2*x1-x2)**2)) + (1/((1-x1-x2)**2))\n",
    "\n",
    "     # Returning the Hessian as a 2x2 matrix\n",
    "    return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We find the descent direction 'd' using the Backtracking-Linear Search Routine or the Armijo Rule.\n",
    "- We check for the following inequality to be true:\n",
    "$$\n",
    "f(x) - f(x+td) < \\alpha(-Df(x)Â·(td))\n",
    "$$\n",
    "where $\\alpha$ is an acceptable descent parameter $\\in (0,0.5)$\n",
    "\n",
    "- We start with t = 1 and then till the about condition holds true, we multiply t with a reduction factor $\\beta$ $\\in (0,1)$\n",
    "\n",
    "- I have implemented the same below. In our case $d$ would be a 1x2 matrix and $Df(x)$ becomes the gradient $\\nabla f$ at the original point $x,y$.\n",
    "\n",
    "- The tweak which we do here to prevent any error from coming up is that we additionally put the inequality conditions on the next $x_1,x_2$ coordinate by ensuring that the step size $t$ and direction $d$ ensures that the next point is in the permissiable region defined by:\n",
    "\n",
    "$$\n",
    "2x_2 - x_1 > 0\n",
    "$$\n",
    "$$\n",
    "2x_1 - x_2 > 0\n",
    "$$\n",
    "$$\n",
    "1 - x_2 - x_1 > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Backtracking Line Search\n",
    "def linear_search(d, alpha, beta, point):\n",
    "    t = 1\n",
    "    while((f(point[0], point[1]) - f(point[0] + t*d[0], point[1] + t*d[1])) < (-alpha*t*np.dot(gradient_f(point[0], point[1]), d))\n",
    "           and (2*(point[0] + t*d[0]) > (point[1] + t*d[1])) and (2*(point[1] + t*d[1]) > (point[0] + t*d[0])) and (point[0] + t*d[0] + point[1] + t*d[1] < 1)): \n",
    "        t = beta*t\n",
    "    return t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We implement the Combination Descent Algorithm below.\n",
    "- First, we have a function `is_pos_def` which checks if a matrix is positive definite by seeing if all its eigenvalues are positive. \n",
    "- This works for a symmetric matrix and we can do this test on the Hessian Matrix $Hf(x)$ because the Hessian Matrix is always symmetric as $\\frac{\\partial^{2} f}{\\partial x_1x_2} = \\frac{\\partial^{2} f}{\\partial x_2x_1}$ (in the context of $R^2$)\n",
    "\n",
    "- If our matrix is positive definite, we choose our $d = (Hf(x))^{-1}(-\\nabla f(x_1,x_2))$ or the $d = -\\nabla f(x_1,x_2)$\n",
    "- Then we apply the Linear Search method to the step-size $t$ which we then use with the desent direction $d$ to update our x and y coordinates.\n",
    "- We perform this in a loop till the value of $\\nabla f(x) \\leq \\eta $ where $\\eta$ is the tolerance level set by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a matrix is positive definite\n",
    "def is_pos_def(x):\n",
    "\n",
    "    # For checking if a matrix is positive definite we check if all the eigenvalues are positive (applicable for symmetric matrices)\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "\n",
    "# Implementing the Combinaion Descent Method\n",
    "def combination_descent(x0, y0, alpha, beta, epsilon, max_iters = 300):\n",
    "    # Initialize the variables\n",
    "    x = x0\n",
    "    y = y0\n",
    "\n",
    "    #Initializing the loop\n",
    "    while (np.linalg.norm(gradient_f(x,y)) > epsilon) and (max_iters != 0):\n",
    "        \n",
    "        # Compute the gradient\n",
    "        grad = gradient_f(x,y)\n",
    "\n",
    "        # Compute the Hessian\n",
    "        hess = hessian_f(x,y)\n",
    "\n",
    "        # Combination Descent Condition:\n",
    "        if(is_pos_def(hess)):\n",
    "\n",
    "            # If the Hessian is positive definite, we use the Newton Step\n",
    "            \n",
    "            # Computing the inverse of the Hessian\n",
    "            hess_inverse = np.linalg.inv(hess)\n",
    "\n",
    "            # Computing the Newton Step by taking the negative of the Hessian inverse dotted with the gradient vector\n",
    "            d = -np.dot(hess_inverse, grad)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # If the Hessian is not positive definite, we use the Gradient Descent Step which is the negative of the gradient vector\n",
    "            d = -grad\n",
    "\n",
    "        # Compute the step size\n",
    "        t = linear_search(d, alpha, beta, [x,y])\n",
    "\n",
    "        # Update the variables\n",
    "        x = x + t*d[0]\n",
    "        y = y + t*d[1]\n",
    "\n",
    "        # Update the number of iterations\n",
    "        max_iters = max_iters - 1\n",
    "\n",
    "    # Return the solution as a numpy array with the number of iterations it took and the coordinates of the solution point\n",
    "    return np.array([x, y, max_iters])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we define the following values:\n",
    "$$\n",
    "(x_1,x_2) = (0.25, 0.25)\n",
    "$$\n",
    "$$\n",
    "\\alpha = 0.1\n",
    "$$\n",
    "$$\n",
    "\\beta = 0.3\n",
    "$$\n",
    "$$\n",
    "\\eta = 10^{-7}\n",
    "$$\n",
    "- We then start the combination descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analytic center of S is at: (0.3333 , 0.3333)\n",
      "The approximate minimum value of the potential function at the analytical center is: 3.2958\n"
     ]
    }
   ],
   "source": [
    "# Define the initial point\n",
    "x0 = 0.25\n",
    "y0 = 0.25\n",
    "\n",
    "# Defining Constants\n",
    "alpha = 0.2\n",
    "beta = 0.5\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Calling the function\n",
    "point = combination_descent(x0, y0, alpha, beta, epsilon)\n",
    "\n",
    "# Printing the results\n",
    "print(\"The analytic center of S is at: (%.4f , %.4f)\" %(point[0], point[1]))\n",
    "print(\"The approximate minimum value of the potential function at the analytical center is: %.4f\" %(f(point[0], point[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get the analytical center for the given potential function approximately for $f(x_1,x_2)$ at $(0.3333, 0.3333)$.\n",
    "- The approximate minimum value of the potential function hence is $3.296$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
